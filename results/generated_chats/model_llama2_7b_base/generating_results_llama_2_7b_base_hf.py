# -*- coding: utf-8 -*-
"""running_llama_2_7b.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14KhH5y42p-AX9jw38YR5nYUWgctIe-00

This code was used to generate the results for model llama2-7b-chat-hf

#Step 1: Generate the prompt dataset
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
#change directory
# %cd /content/drive/MyDrive/DOUTORADO/TESE/code/legalbench

#list the files
!ls

#!git clone https://github.com/HazyResearch/legalbench.git

from tqdm.auto import tqdm
import datasets

from tasks import TASKS, ISSUE_TASKS
from utils import generate_prompts

# Supress progress bars which appear every time a task is downloaded
datasets.utils.logging.set_verbosity_error()

!pip install datasets --upgrade

import os

import pandas as pd

task_name = 'maud_specific_performance'

legalbench_raw_path = "/content/drive/MyDrive/DOUTORADO/TESE/legalbench_huggingface_repo/legalbench/data"

#join task_name to legalbench_raw_directory to form a file path
legalbench_dataset_path = os.path.join(legalbench_raw_path, task_name)

legalbench_dataset_path

os.listdir(legalbench_dataset_path)

#load both train.tsv and test.tsv inside legalbench_dataset_path
train_df = pd.read_csv(os.path.join(legalbench_dataset_path, "train.tsv"), sep="\t")
test_df = pd.read_csv(os.path.join(legalbench_dataset_path, "test.tsv"), sep="\t")

test_df.head()

train_df.head()

# Load base prompt
with open(f"tasks/{task_name}/base_prompt.txt") as in_file:
    prompt_template = in_file.read()
print(prompt_template)

# test_df = dataset["test"].to_pandas()
prompt_dataset = generate_prompts(prompt_template=prompt_template, data_df=test_df)
print(prompt_dataset[0])

"""# Step 2. Generate LLM answers."""

!pip install -q transformers accelerate sentencepiece

!pip install -q transformers huggingface_hub
from huggingface_hub import login

# This will prompt for your access token
login(token="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")  # Get this from huggingface.co/settings/tokens

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# Load model in BF16 precision (best quality within 15GB)
model_id = "meta-llama/Llama-2-7b-chat-hf"

# Load with explicit use_auth_token
tokenizer = AutoTokenizer.from_pretrained(
    model_id,
    use_auth_token=True
)

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    use_auth_token=True
).eval()

#getting the input context size of llama2
model.config

# Get the newline token ID
# newline_token_id = tokenizer("\n")["input_ids"][-1]

# def generate_deterministic(prompt):
#     """Generate text with:
#     - Temperature 0.0 (fully deterministic)
#     - Stops at newline
#     - No sampling warnings"""

#     inputs = tokenizer(prompt, return_tensors="pt").to("cuda")

#     # Generation parameters for deterministic output
#     outputs = model.generate(
#         **inputs,
#         max_new_tokens=100,
#         do_sample=False,  # Greedy decoding
#         temperature=None,  # Not used when do_sample=False
#         top_p=None,  # Not used when do_sample=False
#         eos_token_id=newline_token_id,  # Stop at newline
#         pad_token_id=tokenizer.eos_token_id
#     )

#     # Clean output - remove prompt and keep only generation
#     full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
#     return full_text[len(prompt):].split("\n")[0]  # Extract only new content

# Get the newline token ID - more robust approach
newline_token_id = tokenizer.encode("\n", add_special_tokens=False)[-1]

def generate_deterministic(prompt):
    """Generate text with:
    - Temperature 0.0 (fully deterministic)
    - Stops at newline
    - No sampling warnings"""

    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")

    # Generation parameters for deterministic output
    outputs = model.generate(
        **inputs,
        max_new_tokens=100,
        do_sample=False,  # Greedy decoding. Llama doesnt allow temperature = 0.0
        eos_token_id=newline_token_id,  # Stop at newline
        pad_token_id=tokenizer.eos_token_id
    )

    # Decode only the new tokens (excluding prompt)
    generated_ids = outputs[0][len(inputs["input_ids"][0]):]
    return tokenizer.decode(generated_ids, skip_special_tokens=True).split("\n")[0]

generate_deterministic(prompt_dataset[0])

# Process your dataset
generations = []
for prompt in prompt_dataset:
    torch.cuda.empty_cache()  # Clear memory between generations
    generations.append(generate_deterministic(prompt))

from sklearn.metrics import balanced_accuracy_score
import pandas as pd
import numpy as np

# Clean and convert labels
def clean_label(label):
    if pd.isna(label) or str(label).strip() == '':
        return np.nan  # Will be filtered out
    cleaned = str(label).strip().lower()
    return 1 if cleaned in ('yes', 'y') else 0 if cleaned in ('no', 'n') else np.nan

# Convert to numeric (1=Yes, 0=No)
y_true = test_df['answer'].apply(clean_label)
y_pred = pd.Series(generations).apply(clean_label)

# Filter out invalid entries (NaN)
valid_mask = ~(np.isnan(y_true) | np.isnan(y_pred))
y_true_clean = y_true[valid_mask]
y_pred_clean = y_pred[valid_mask]

# Calculate balanced accuracy
if len(y_true_clean) > 0:
    balanced_acc = balanced_accuracy_score(y_true_clean, y_pred_clean)
    print(f"Balanced Accuracy: {balanced_acc:.4f}")
    print(f"Valid samples: {len(y_true_clean)}/{len(y_true)}")
else:
    print("Error: No valid samples after cleaning!")

"""# Step 3. Evaluate LLM answers."""

from evaluation import evaluate

evaluate(task_name, generations, test_df["answer"].tolist())

generated_chat_path = "/content/drive/MyDrive/DOUTORADO/TESE/Experimento_LegalBench_1/generated_chats/model_llama2_7b"

model_name = "Llama_2_7b_chat_hf"

task_group = "LJT"

# join task_name and model_name to form the name of generated_chat file
generated_chat_file = os.path.join(generated_chat_path, f"generated_chat_{task_name}_{model_name}_{task_group}.csv")

generated_chat_file

#save generations to csv
# import pandas as pd
df = pd.DataFrame(generations)
df.to_csv(generated_chat_file, index=False)

df

# import os
# import pandas as pd

# Your GitHub-tracked directory (where files should be saved)
# GIT_TRACKED_DIR = "/content/drive/MyDrive/DOUTORADO/TESE/my_git_repo/subLegalBench_experiment_1/results/generated_chats/model_llama2_7b"

# filename = f"generated_chat_{task_name}_{model_name}.csv"

# pd.DataFrame(generations).to_csv(os.path.join(GIT_TRACKED_DIR, filename), index=False)

# ===== 3. GIT COMMIT =====
# Navigate to repo root
# REPO_ROOT = "/content/drive/MyDrive/DOUTORADO/TESE/my_git_repo/subLegalBench_experiment_1"
# os.chdir(REPO_ROOT)

# !apt-get install git -y
# !git config --global core.hooksPath /dev/null  # Disable problematic hooks
# !git config --global user.email "israelfama@yahoo.com"
# !git config --global user.name "israelfama"

# Git operations (YOUR CORRECTED PATH)
# !git add results/generated_chats/model_llama2_7b/{filename}

# !git commit -m "Auto-commit: {task_name} results ({timestamp})"

# !git push origin main